{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named graphlab",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e38e24962b83>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mgraphlab\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named graphlab"
     ]
    }
   ],
   "source": [
    "import graphlab as gl\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Train = gl.SFrame.read_csv('train_int.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train = gl.toolkits.cross_validation.shuffle(Train)\n",
    "folds = gl.toolkits.cross_validation.KFold(Train,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "features = Train.column_names()\n",
    "for x in ['id','loss','X1']:\n",
    "    features.remove(x)\n",
    "for ttrain,valid in folds:\n",
    "    model = gl.boosted_trees_regression.create(ttrain,target='loss',features=features, validation_set=valid,max_iterations=21,max_depth=15,verbose=False)\n",
    "    print model.evaluate(ttrain)\n",
    "    print model.evaluate(valid)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Test = gl.SFrame.read_csv('test_int.csv',verbose=False)\n",
    "sub = None\n",
    "te = None\n",
    "for model in models:\n",
    "    if sub is None:\n",
    "        sub = model.predict(Test)\n",
    "        te = model.predict(Train)\n",
    "    else:\n",
    "        temp = np.array(model.predict(Test))\n",
    "        temp2 = np.array(model.predict(Train))\n",
    "        sub = np.array(sub)\n",
    "        te = np.array(te)\n",
    "        sub = (sub+temp)\n",
    "        te = (te+temp2)\n",
    "sub = sub/5.0\n",
    "te = te/5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "def evalerror(pred, dtrain):\n",
    "    labels = dtrain\n",
    "    return 'mae',mean_absolute_error(te,labels)\n",
    "\n",
    "print evalerror(te,np.array(Train['loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(Test),len(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Test['loss'] = sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submit = Test[['id','loss']]\n",
    "submit.save('submit1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for train,valid in folds:\n",
    "    for i in range(6):\n",
    "        print 20*i+1, 'iterations:'\n",
    "        model = gl.boosted_trees_regression.create(train,target='loss',validation_set=valid,max_iterations=20*i+1,max_depth=15,verbose=False)\n",
    "        print model.evaluate(train)\n",
    "        print model.evaluate(valid)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Train = gl.toolkits.cross_validation.shuffle(Train)\n",
    "folds = gl.toolkits.cross_validation.KFold(Train,5)\n",
    "models = []\n",
    "for train,valid in folds:\n",
    "    model = gl.boosted_trees_regression.create(train,target='loss',validation_set=valid,max_iterations=21,max_depth=15,verbose=False)\n",
    "    print model.evaluate(train)\n",
    "    print model.evaluate(valid)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "Test = gl.SFrame.read_csv('test.csv',verbose=False)\n",
    "sub = None\n",
    "for model in models:\n",
    "    if sub is None:\n",
    "        sub = model.predict(Test)\n",
    "    else:\n",
    "        temp = np.array(model.predict(Test))\n",
    "        sub = np.array(sub)\n",
    "        sub = (sub+temp)\n",
    "sub = sub/5.0\n",
    "Test['loss'] = sub\n",
    "submit = Test[['id','loss']]\n",
    "submit.save('submit3.csv.gz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quit Graphlab, head to pandas and XGB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "test['loss'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evalerror(pred, dtrain):\n",
    "    labels = dtrain.get_label()\n",
    "    return 'mae',mean_absolute_error(np.exp(pred),np.exp(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for column in list(train.select_dtypes(include=['object']).columns):\n",
    "    if train[column].nunique() != test[column].nunique():\n",
    "        set_train = set(train[column].unique())\n",
    "        set_test = set(test[column].unique())\n",
    "        remove_train = set_train - set_test\n",
    "        remove_test = set_test - set_train\n",
    "        \n",
    "        remove = remove_train.union(remove_test)\n",
    "        def filter_cat(x):\n",
    "            if x in remove:\n",
    "                return np.nan\n",
    "            return x\n",
    "            \n",
    "        joined[column] = joined[column].apply(lambda x:filter_cat(x),1)\n",
    "        \n",
    "    joined[column] = pd.factorize(joined[column].values,sort=True)[0]\n",
    "\n",
    "train = joined[joined['loss'].notnull()]\n",
    "test = joined[joined['loss'].isnull()]\n",
    "\n",
    "shift = 200\n",
    "\n",
    "y = np.log(train['loss'] + shift)\n",
    "ids = test['id']\n",
    "X = train.drop(['id','loss'],1)\n",
    "X_test = test.drop(['id','loss'],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 2016\n",
    "params = {\n",
    "    'min_child_weight':1,\n",
    "    'eta':0.05,\n",
    "    'colsample_bytree':0.5,\n",
    "    'max_depth':12,\n",
    "    'subsample':0.8,\n",
    "    'alpha':1,\n",
    "    'gamma':1,\n",
    "    'silent':0,\n",
    "    'verbose_eval':10,\n",
    "    'seed':RANDOM_STATE\n",
    "}\n",
    "\n",
    "xgtrain = xgb.DMatrix(X,label=y)\n",
    "xgtest = xgb.DMatrix(X_test)\n",
    "\n",
    "eval_set = [(xgtrain,'train')]\n",
    "\n",
    "model = xgb.train(params, xgtrain, 30, feval=evalerror)\n",
    "\n",
    "prediction = np.exp(model.predict(xgtest)) - shift\n",
    "\n",
    "submission = pd.DataFrame()\n",
    "submission['loss'] = prediction\n",
    "submission['id'] = ids\n",
    "submission.to_csv('submit4.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from xgboost import plot_tree\n",
    "%matplotlib inline\n",
    "plot_tree(model,num_trees=0,rankdir='LR')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 2016\n",
    "params = {\n",
    "    'min_child_weight':1,\n",
    "    'eta':0.1,\n",
    "    'colsample_bytree':0.5,\n",
    "    'max_depth':12,\n",
    "    'subsample':0.8,\n",
    "    'alpha':1,\n",
    "    'gamma':1,\n",
    "    'silent':0,\n",
    "    'seed':RANDOM_STATE\n",
    "}\n",
    "\n",
    "xgtrain = xgb.DMatrix(X,label=y)\n",
    "xgtest = xgb.DMatrix(X_test)\n",
    "\n",
    "eval_set = [(xgtrain,'eval')]\n",
    "\n",
    "model = xgb.train(params, xgtrain, 5000,eval_set,verbose_eval=50, feval=evalerror, early_stopping_rounds=30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 2016\n",
    "params = {\n",
    "    'min_child_weight':1,\n",
    "    'eta':0.05,\n",
    "    'colsample_bytree':0.5,\n",
    "    'max_depth':12,\n",
    "    'subsample':0.5,\n",
    "    'alpha':1,\n",
    "    'gamma':1,\n",
    "    'silent':1,\n",
    "    'seed':RANDOM_STATE\n",
    "}\n",
    "\n",
    "xgtrain = xgb.DMatrix(X,label=y)\n",
    "xgtest = xgb.DMatrix(X_test)\n",
    "\n",
    "pdata = xgb.cv(params, xgtrain, 5000,verbose_eval= 10, feval=evalerror, nfold=5, early_stopping_rounds=20)\n",
    "\n",
    "pdata.to_csv('Pdata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 2016\n",
    "params = {\n",
    "    'min_child_weight':1,\n",
    "    'eta':0.05,\n",
    "    'colsample_bytree':0.5,\n",
    "    'max_depth':12,\n",
    "    'subsample':0.5,\n",
    "    'alpha':1,\n",
    "    'gamma':1,\n",
    "    'silent':1,\n",
    "    'seed':RANDOM_STATE\n",
    "}\n",
    "\n",
    "xgtrain = xgb.DMatrix(X,label=y)\n",
    "xgtest = xgb.DMatrix(X_test)\n",
    "\n",
    "pdata = xgb.cv(params, xgtrain, 1000,verbose_eval= 10, feval=evalerror, nfold=5)\n",
    "\n",
    "pdata.to_csv('Pdata2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 2016\n",
    "params = {\n",
    "    'min_child_weight':1,\n",
    "    'eta':0.05,\n",
    "    'colsample_bytree':0.5,\n",
    "    'max_depth':12,\n",
    "    'subsample':0.8,\n",
    "    'alpha':1,\n",
    "    'gamma':1,\n",
    "    'silent':1,\n",
    "    'seed':RANDOM_STATE\n",
    "}\n",
    "\n",
    "xgtrain = xgb.DMatrix(X,label=y)\n",
    "xgtest = xgb.DMatrix(X_test)\n",
    "\n",
    "pdata = xgb.cv(params, xgtrain, 1000,verbose_eval= 10, feval=evalerror, nfold=5)\n",
    "\n",
    "pdata.to_csv('Pdata3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 2016\n",
    "params = {\n",
    "    'min_child_weight':1,\n",
    "    'eta':0.01,\n",
    "    'colsample_bytree':0.5,\n",
    "    'max_depth':12,\n",
    "    'subsample':0.5,\n",
    "    'alpha':1,\n",
    "    'gamma':1,\n",
    "    'silent':1,\n",
    "    'seed':RANDOM_STATE\n",
    "}\n",
    "\n",
    "xgtrain = xgb.DMatrix(X,label=y)\n",
    "xgtest = xgb.DMatrix(X_test)\n",
    "\n",
    "pdata = xgb.cv(params, xgtrain, 3000,verbose_eval= 10, feval=evalerror, nfold=5)\n",
    "\n",
    "pdata.to_csv('Pdata2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 2016\n",
    "params = {\n",
    "    'min_child_weight':1,\n",
    "    'eta':0.01,\n",
    "    'colsample_bytree':0.5,\n",
    "    'max_depth':12,\n",
    "    'subsample':0.8,\n",
    "    'alpha':1,\n",
    "    'gamma':1,\n",
    "    'silent':1,\n",
    "    'seed':RANDOM_STATE\n",
    "}\n",
    "\n",
    "xgtrain = xgb.DMatrix(X,label=y)\n",
    "xgtest = xgb.DMatrix(X_test)\n",
    "\n",
    "pdata = xgb.cv(params, xgtrain, 3000,verbose_eval= 10, feval=evalerror, nfold=10)\n",
    "\n",
    "pdata.to_csv('Pdata2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RANDOM_STATE = 2016\n",
    "params = {\n",
    "    'min_child_weight':1,\n",
    "    'eta':0.01,\n",
    "    'colsample_bytree':0.6,\n",
    "    'max_depth':12,\n",
    "    'subsample':0.8,\n",
    "    'alpha':1,\n",
    "    'gamma':1,\n",
    "    'silent':1,\n",
    "    'seed':RANDOM_STATE\n",
    "}\n",
    "\n",
    "xgtrain = xgb.DMatrix(X,label=y)\n",
    "xgtest = xgb.DMatrix(X_test)\n",
    "\n",
    "pdata = xgb.cv(params, xgtrain, 3000,verbose_eval= 10, feval=evalerror, nfold=20)\n",
    "\n",
    "pdata.to_csv('Pdata3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.regularizers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = list(train.columns)\n",
    "for x in ['id','loss']:\n",
    "    features.remove(x)\n",
    "X = train[features].as_matrix()\n",
    "Y = np.log(train['loss'].as_matrix()+shift)\n",
    "X_test = test[features].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras import backend as K\n",
    "def custom_mae(y_true,y_pred):\n",
    "    return K.mean(K.abs(K.exp(y_pred) - K.exp(y_true)), axis=-1)\n",
    "\n",
    "def custome_rmse(y_true,y_pred):\n",
    "    return K.sqrt(K.mean(K.square(K.exp(y_pred) - K.exp(y_true)), axis=-1))\n",
    "\n",
    "def custom_mse(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true), axis=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test['loss'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "joined = pd.concat([train,test])\n",
    "for column in features:\n",
    "    mn = min(joined[column])\n",
    "    mx = max(joined[column])\n",
    "    joined[column] = joined[column].apply(lambda x:(x-mn)*1.0/(mx-mn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = joined[joined['loss'].notnull()]\n",
    "test = joined[joined['loss'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print len(train),len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = list(train.columns)\n",
    "for x in ['id','loss']:\n",
    "    features.remove(x)\n",
    "X = train[features].as_matrix()\n",
    "Y = train['loss'].as_matrix()\n",
    "X_test = test[features].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.python.control_flow_ops = tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(260,input_dim=130,init='uniform',activation='relu',W_regularizer=l1l2(l1=0.1,l2=0.1),activity_regularizer=activity_l1l2(l1=0.1,l2=0.1)))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(260,init='uniform',activation='relu',W_regularizer=l1l2(l1=0.1,l2=0.1),activity_regularizer=activity_l1l2(l1=0.1,l2=0.1)))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(1,init='uniform',activation='linear'))\n",
    "model.compile(loss=custom_mae,optimizer='adam')\n",
    "\n",
    "##Saving checkpoints\n",
    "filepath=\"weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(260,input_dim=130,init='uniform',activation='relu'))\n",
    "model.add(Dense(260,init='uniform',activation='relu'))\n",
    "model.add(Dense(1,init='uniform',activation='linear'))\n",
    "model.compile(loss=custom_mae,optimizer='adam')\n",
    "\n",
    "##Saving checkpoints\n",
    "filepath=\"weights.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X,Y,nb_epoch=200,batch_size=10,callbacks = callbacks_list, validation_split=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def np_mae(pred,true):\n",
    "    return mean_absolute_error(np.exp(true),np.exp(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np_mae(model.predict(X),Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Keras Starter by danijelk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(123)\n",
    "import pandas as pd\n",
    "import subprocess\n",
    "from scipy.sparse import csr_matrix,hstack\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import KFold\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.callbacks import CSVLogger,EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Generator for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##Manually generate batches based on chenglong discussion\n",
    "\n",
    "def batch_generator(X,Y,batch_size,shuffle):\n",
    "    number_of_batches = np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:]\n",
    "        Y_batch = Y[batch_index]\n",
    "        counter += 1\n",
    "        yield X_batch,Y_batch\n",
    "        if counter == number_of_batches:\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch generator for testing purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_generatorp(X, batch_size, shuffle):\n",
    "    number_of_batches = X.shape[0]/np.ceil(X.shape[0]/batch_size)\n",
    "    counter = 0\n",
    "    sample_index = np.arange(X.shape[0])\n",
    "    if shuffle:\n",
    "        np.random.shuffle(sample_index)\n",
    "    while True:\n",
    "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X[batch_index,:]\n",
    "        counter += 1\n",
    "        yield X_batch\n",
    "        if counter == number_of_batches:\n",
    "            if shuffle:\n",
    "                np.random.shuffle(sample_index)\n",
    "            counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train :  188318  Legth of Test :  125546\n"
     ]
    }
   ],
   "source": [
    "print 'Length of Train : ', len(train), ' Legth of Test : ',len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test['loss'] = np.nan\n",
    "Y = train['loss'].as_matrix()\n",
    "id_train = train['id'].as_matrix()\n",
    "id_test = test['id'].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ntrain = train.shape[0]\n",
    "joined = pd.concat([train,test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## categorical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313864\n"
     ]
    }
   ],
   "source": [
    "f_cat = [f for f in joined.columns if 'cat' in f]    \n",
    "tmp_cat = None\n",
    "for f in f_cat:\n",
    "    if tmp_cat is None:\n",
    "        tmp_cat = pd.get_dummies(joined[f].astype('category'),prefix=f)\n",
    "    tmp_cat = pd.concat([tmp_cat,pd.get_dummies(joined[f].astype('category'),prefix=f)],axis=1)\n",
    "print tmp_cat.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Numerical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313864\n"
     ]
    }
   ],
   "source": [
    "f_num = [f for f in joined.columns if 'cont' in f]\n",
    "scaler = StandardScaler()\n",
    "tmp_num = pd.DataFrame(scaler.fit_transform(joined[f_num]),columns=f_num).set_index(joined.index)\n",
    "print tmp_num.shape[0]\n",
    "tmp = pd.concat([tmp_cat,tmp_num],axis=1)\n",
    "tmp['loss'] = joined['loss']\n",
    "del(tmp_cat,tmp_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(313864, 1193)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dim Train :  188318 Dim Test :  125546\n"
     ]
    }
   ],
   "source": [
    "joined = tmp\n",
    "train = joined[joined['loss'].notnull()]\n",
    "test = joined[joined['loss'].isnull()]\n",
    "print 'Dim Train : ',train.shape[0], 'Dim Test : ',test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = list(train.columns)\n",
    "features.remove('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = train[features].as_matrix()\n",
    "Y_train = train['loss'].as_matrix()\n",
    "Shift = 200\n",
    "Y_train = np.log(Y_train+Shift)\n",
    "X_test = test[features].as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del(joined,train,test,tmp)\n",
    "##Remaining data points are X_train, Y_train, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custome evaluation function for keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "def custom_mae(y_true,y_pred):\n",
    "    return K.mean(K.abs(K.exp(y_pred) - K.exp(y_true)), axis=-1)\n",
    "\n",
    "def custome_rmse(y_true,y_pred):\n",
    "    return K.sqrt(K.mean(K.square(K.exp(y_pred) - K.exp(y_true)), axis=-1))\n",
    "\n",
    "def custom_mse(y_true, y_pred):\n",
    "    return K.mean(K.square(y_pred - y_true), axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining NN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#tf.python.control_flow_ops = tf\n",
    "def nn_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(400,input_dim=X_train.shape[1],init='he_normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(200,init='he_normal'))\n",
    "    model.add(PReLU())\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1,init='he_normal'))\n",
    "    model.compile(loss=custom_mae,optimizer='adadelta')\n",
    "    return model\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = nn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CV returning indexes\n",
    "nfolds = 6\n",
    "folds = KFold(len(Y_train),n_folds=nfolds, shuffle=True,random_state=2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st bag of  1  Fold took :  165.271440983  seconds\n",
      "2033.41951813\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-a614584b3436>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mShift\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mpred\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_generatorp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mval_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mShift\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mpred_test\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/him/anaconda2/envs/theano-gpu/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(self, generator, val_samples, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m    954\u001b[0m                                             \u001b[0mmax_q_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_q_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m                                             \u001b[0mnb_worker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_worker\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m                                             pickle_safe=pickle_safe)\n\u001b[0m\u001b[1;32m    957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/him/anaconda2/envs/theano-gpu/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mpredict_generator\u001b[0;34m(self, generator, val_samples, max_q_size, nb_worker, pickle_safe)\u001b[0m\n\u001b[1;32m   1643\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1644\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1645\u001b[0;31m                     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1647\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Training Model\n",
    "i=0\n",
    "nbags = 5\n",
    "nepochs = 60\n",
    "pred_oob = np.zeros(X_train.shape[0])\n",
    "pred_test = np.zeros(X_test.shape[0])\n",
    "gModel = []\n",
    "\n",
    "for (inTr, inTe) in folds:\n",
    "    X = X_train[inTr]\n",
    "    Y = Y_train[inTr]\n",
    "    X_val = X_train[inTe]\n",
    "    Y_val = Y_train[inTe]\n",
    "    pred = np.zeros(X_val.shape[0])\n",
    "    import time\n",
    "    start_time = time.time()\n",
    "    for j in range(nbags):\n",
    "        with open('log.txt','ab') as myfile:\n",
    "            myfile.write('Beginning Training Fold '+str(i+1)+' bag '+str(j+1)+'\\n')\n",
    "            \n",
    "        ##CSV Logger ... closes after all callbacks, thus local\n",
    "        csv_logger = CSVLogger('log.txt',append=True)\n",
    "        model = nn_model()\n",
    "        mtime = time.time() ##model time measure for each folds first bag        \n",
    "        ## Using early stopping for automatic stopping\n",
    "        earlyStopping=EarlyStopping(monitor='val_loss', patience=5, verbose=0, mode='min')\n",
    "        fit = model.fit_generator(generator=batch_generator(X,Y,128,True),\n",
    "                                  validation_data=(X_val,Y_val),\n",
    "                                  callbacks=[csv_logger,earlyStopping],\n",
    "                                  nb_epoch=200,\n",
    "                                  samples_per_epoch=X.shape[0],\n",
    "                                  verbose=0)\n",
    "        gModel.append(model)\n",
    "        model.save('models/model'+str(i)+'_'+str(j)+'.h5')\n",
    "        with open('log.txt','ab') as myfile:\n",
    "            myfile.write('Done Training Fold '+str(i+1)+' bag '+str(j+1)+'\\n')\n",
    "        if j==0:\n",
    "            print '1st bag of ',i+1,' Fold took : ',time.time()-mtime,' seconds'\n",
    "            print mean_absolute_error(np.exp(model.predict_generator(generator=batch_generatorp(X_val,800,False),val_samples=X_val.shape[0])[:,0])-Shift,np.exp(Y_val)-Shift)\n",
    "        temp = model.predict_generator(generator=batch_generatorp(X_val,800,False),val_samples=X_val.shape[0])[:,0]\n",
    "        temp = np.exp(temp) - Shift\n",
    "        pred += temp\n",
    "        temp = model.predict_generator(generator=batch_generatorp(X_test,800,False),val_samples=X_test.shape[0])[:,0]\n",
    "        temp = np.exp(temp) - Shift\n",
    "        pred_test += temp\n",
    "    print i+1, ' Fold took ',time.time()-start_time,' seconds '\n",
    "    pred /= nbags\n",
    "    pred_oob[inTe] = pred\n",
    "    score = mean_absolute_error(np.exp(Y_val)-Shift,pred)\n",
    "    i += 1\n",
    "    print 'Fold ',i,'-MAE:',score\n",
    "    \n",
    "print 'Total - MAE: ',mean_absolute_error(np.exp(Y_train)-Shift,pred_oob)\n",
    "\n",
    "##train predictions\n",
    "df = pd.DataFrame({'id':id_train,'loss':pred_oob})\n",
    "df.to_csv('Pred_OOB_keras.csv',index=False)\n",
    "\n",
    "##test predictions\n",
    "pred_test /= nbags*nfolds\n",
    "df = pd.DataFrame({'id':id_test,'loss':pred_test})\n",
    "df.to_csv('submission_keras.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:theano-gpu]",
   "language": "python",
   "name": "conda-env-theano-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
